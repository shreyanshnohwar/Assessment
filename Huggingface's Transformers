from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

def generate_text(prompt, temperature, k):
    # Load pre-trained model and tokenizer
    model_name = "gpt2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    # Encode the input prompt
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # Generate text
    output = model.generate(
        input_ids,
        max_length=50 + len(input_ids[0]),  # ensure 50 new tokens
        do_sample=True,
        top_k=k,
        temperature=temperature,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # Decode output
    return tokenizer.decode(output[0], skip_special_tokens=True)

if __name__ == "__main__":
    prompt = "Once upon a time"
    output_07 = generate_text(prompt, temperature=0.7, k=50)
    output_10 = generate_text(prompt, temperature=1.0, k=50)
    
    # Save both outputs to a file
    with open("generated_outputs.txt", "w", encoding="utf-8") as f:
        f.write("=== Output with Temperature 0.7 ===\n")
        f.write(output_07 + "\n\n")
        f.write("=== Output with Temperature 1.0 ===\n")
        f.write(output_10)
